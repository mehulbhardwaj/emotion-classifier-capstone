#!/usr/bin/env python3
"""
Comprehensive debug script to test all three architectures.
Works in both local and Google Colab environments.
"""

import sys
import torch
import numpy as np
from pathlib import Path

# Add current directory to path for imports
sys.path.append(str(Path.cwd()))

def create_test_config():
    """Create a minimal test configuration."""
    class TestConfig:
        def __init__(self):
            # Core settings
            self.architecture_name = "mlp_fusion"  # Will be changed per test
            self.output_dim = 7
            self.batch_size = 2
            
            # Model settings
            self.text_encoder_model_name = "roberta-base"
            self.audio_encoder_model_name = "facebook/wav2vec2-base-960h"
            self.audio_input_type = "raw_wav"
            self.text_max_len = 128
            
            # Training settings
            self.learning_rate = 1e-4
            self.num_epochs = 10
            self.focal_gamma = 2.0
            self.weight_decay = 1e-4
            
            # Architecture-specific settings
            self.mlp_hidden_size = 256
            self.gru_hidden_size = 128
            self.context_window = 0
            self.topic_embedding_dim = 100
            self.use_knowledge = False
            self.n_topics = 50
            self.rel_transformer_layers = 2
            self.rel_heads = 4
            
            # Class weights (balanced for testing)
            self.class_weights = [1.0] * self.output_dim
            
            # Data paths (will be set based on environment)
            self.data_root = Path("/content/drive/MyDrive/dlfa_capstone")  # Colab default
            if not self.data_root.exists():
                self.data_root = Path("./meld_data")  # Local fallback
            
            self.dataloader_num_workers = 0  # Safe for testing
            
    return TestConfig()

def create_dummy_data(config, arch_name):
    """Create dummy data for testing when real data isn't available."""
    B = config.batch_size
    
    if arch_name in ["dialog_rnn", "todkat_lite"]:
        # Dialog-level data (B, T, L)
        T = 3  # 3 utterances per dialog
        L_audio = 16000  # 1 second of 16kHz audio
        L_text = 50  # 50 tokens
        
        dummy_batch = {
            "wav": torch.randn(B, T, L_audio),
            "wav_mask": torch.ones(B, T, L_audio, dtype=torch.long),
            "txt": torch.randint(0, 1000, (B, T, L_text)),
            "txt_mask": torch.ones(B, T, L_text, dtype=torch.long),
            "labels": torch.randint(0, config.output_dim, (B, T)),
            "speaker_id": torch.randint(0, 5, (B, T)),  # 5 different speakers
            "dialog_mask": torch.ones(B, T, dtype=torch.bool),
        }
        
        if arch_name == "todkat_lite":
            dummy_batch["topic_id"] = torch.randint(0, config.n_topics, (B, T))
            dummy_batch["kn_vec"] = torch.randn(B, T, 50)
            
    else:
        # Utterance-level data (5-tuple format)
        L_audio = 16000
        L_text = 50
        
        dummy_batch = (
            torch.randn(B, L_audio),  # wav
            torch.ones(B, L_audio, dtype=torch.long),  # wav_mask
            torch.randint(0, 1000, (B, L_text)),  # txt
            torch.ones(B, L_text, dtype=torch.long),  # txt_mask
            torch.randint(0, config.output_dim, (B,))  # labels
        )
    
    return dummy_batch

def test_architecture(arch_name, use_dummy_data=False):
    """Test a specific architecture."""
    print(f"\n{'='*60}")
    print(f"Testing {arch_name.upper()}")
    print(f"{'='*60}")
    
    # Create config
    config = create_test_config()
    config.architecture_name = arch_name
    
    # Import models
    try:
        from models.mlp_fusion import MultimodalFusionMLP
        from models.dialog_rnn import DialogRNNMLP
        from models.todkat_lite import TodkatLiteMLP
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False
    
    # Create model
    try:
        if arch_name == "mlp_fusion":
            model = MultimodalFusionMLP(config)
        elif arch_name == "dialog_rnn":
            model = DialogRNNMLP(config)
        elif arch_name == "todkat_lite":
            model = TodkatLiteMLP(config)
        else:
            raise ValueError(f"Unknown architecture: {arch_name}")
        
        print(f"‚úÖ Model created successfully")
        print(f"   Model type: {type(model).__name__}")
        print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
        
    except Exception as e:
        print(f"‚ùå Model creation failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Test with data
    try:
        if use_dummy_data:
            print("   Using dummy data...")
            batch = create_dummy_data(config, arch_name)
        else:
            print("   Attempting to load real data...")
            try:
                from utils.data_processor import MELDDataModule
                data_module = MELDDataModule(config)
                data_module.setup()
                train_loader = data_module.train_dataloader()
                batch = next(iter(train_loader))
                print("   ‚úÖ Real data loaded successfully")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Real data failed: {e}")
                print("   Falling back to dummy data...")
                batch = create_dummy_data(config, arch_name)
        
        # Print batch info
        if isinstance(batch, dict):
            print(f"   Batch format: dict with keys {list(batch.keys())}")
            for key, value in batch.items():
                if torch.is_tensor(value):
                    print(f"     {key}: {value.shape}")
        else:
            print(f"   Batch format: tuple with {len(batch)} elements")
            for i, item in enumerate(batch):
                if torch.is_tensor(item):
                    print(f"     Item {i}: {item.shape}")
        
    except Exception as e:
        print(f"‚ùå Data loading failed: {e}")
        return False
    
    # Test forward pass
    try:
        model.eval()
        with torch.no_grad():
            if arch_name == "mlp_fusion":
                wav, wav_mask, txt, txt_mask, labels = batch
                logits = model(wav, wav_mask, txt, txt_mask)
            else:
                logits = model(batch)
            
            print(f"‚úÖ Forward pass successful")
            print(f"   Output shape: {logits.shape}")
            print(f"   Expected classes: {config.output_dim}")
            
            # Check output validity
            if logits.shape[-1] != config.output_dim:
                print(f"‚ö†Ô∏è  Warning: Output dimension mismatch!")
                return False
                
            # Check for NaN/Inf
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                print(f"‚ö†Ô∏è  Warning: NaN or Inf in outputs!")
                return False
                
            # Test backward pass
            if arch_name == "mlp_fusion":
                loss = torch.nn.functional.cross_entropy(logits, labels)
            else:
                if arch_name == "dialog_rnn":
                    mask = batch["dialog_mask"].bool()
                    labels_flat = batch["labels"][mask]
                    logits_flat = logits[mask]
                    loss = torch.nn.functional.cross_entropy(logits_flat, labels_flat)
                else:  # todkat_lite
                    labels_last = batch["labels"][:, -1]
                    loss = torch.nn.functional.cross_entropy(logits, labels_last)
            
            print(f"   Loss: {loss.item():.4f}")
            
    except Exception as e:
        print(f"‚ùå Forward pass failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print(f"‚úÖ {arch_name.upper()} test completed successfully!")
    return True

def main():
    """Test all architectures."""
    print("üß™ Testing all model architectures...")
    print("üè† Working directory:", Path.cwd())
    print("üêç Python path:", sys.path[:3])  # First 3 entries
    
    architectures = ["mlp_fusion", "dialog_rnn", "todkat_lite"]
    results = {}
    
    # Try with real data first, then fallback to dummy data
    for use_dummy in [False, True]:
        if use_dummy:
            print("\n" + "="*60)
            print("FALLBACK: Testing with dummy data")
            print("="*60)
        
        for arch in architectures:
            if arch in results and results[arch]:
                continue  # Skip if already passed
            results[arch] = test_architecture(arch, use_dummy_data=use_dummy)
        
        # If all passed, break
        if all(results.values()):
            break
    
    # Summary
    print(f"\n{'='*60}")
    print("FINAL SUMMARY")
    print(f"{'='*60}")
    
    for arch, success in results.items():
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{arch:<15}: {status}")
    
    all_passed = all(results.values())
    if all_passed:
        print("\nüéâ All architectures working correctly!")
        print("   Your original issue (identical F1 scores) should now be fixed.")
        print("   The key fixes were:")
        print("   1. ‚úÖ train.py now uses correct model classes")
        print("   2. ‚úÖ data_processor.py uses correct attribute names")
        print("   3. ‚úÖ todkat_lite.py scheduler bug fixed")
        print("\n   You should now see different F1 scores for each architecture!")
    else:
        print("\n‚ö†Ô∏è  Some architectures failed. Check the errors above.")
        failed = [arch for arch, success in results.items() if not success]
        print(f"   Failed architectures: {failed}")
    
    return all_passed

if __name__ == "__main__":
    main() 