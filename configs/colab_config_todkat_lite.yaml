# TOD-KAT Lite Config - Optimized for 7.5M Parameters
# ===================================================

# CRITICAL: Architecture selection
architecture_name: todkat_lite

# Experiment settings
experiment_name: colab_run_todkat_lite_optimal
random_seed: 42

# Data paths
data_root: /content/drive/MyDrive/dlfa_capstone/meld_data

# Model architecture settings - OPTIMIZED FOR 7.5M PARAMS
output_dim: 7
topic_embedding_dim: 64      # INCREASED: More topic representation
n_topics: 64                 # INCREASED: More topic diversity
rel_transformer_layers: 3    # INCREASED: Deeper reasoning (key for dialogue)
rel_heads: 8                 # INCREASED: More attention patterns
projection_dim: 256          # OPTIMIZED: Balance size vs capacity
mlp_hidden_size: 1024        # INCREASED: Larger classifier capacity
mlp_dropout_rate: 0.3
use_knowledge: true          # CORE: Knowledge is essential for TOD-KAT
knowledge_dim: 32            # INCREASED: Richer knowledge representation

# TOD-KAT Specific Features
use_topic_mlps: true         # NEW: Add topic MLPs (fₙᵤ, fₛ) like SOTA
use_knowledge_attention: true # NEW: Bahdanau-style knowledge attention
transformer_dim_feedforward: 512  # INCREASED: More transformer capacity

# Encoder settings
text_encoder_model_name: roberta-base
audio_encoder_model_name: facebook/wav2vec2-base-960h
audio_input_type: raw_wav
freeze_text_encoder: true
freeze_audio_encoder: true

# Training settings
batch_size: 8
num_epochs: 25
learning_rate: 1e-4
weight_decay: 1e-4
optimizer_name: AdamW

# Loss settings
focal_gamma: 2.0

# Class weights for MELD emotion imbalance
class_weights: [1.62, 8.22, 11.19, 1.40, 0.45, 2.69, 2.00]

# Training options
use_mixed_precision: true
early_stopping_patience: 5
grad_accumulation_steps: 1

# DataLoader settings
dataloader_num_workers: 2

# Text processing
text_max_length: 128

# Scheduler settings
eta_min: 1e-7 