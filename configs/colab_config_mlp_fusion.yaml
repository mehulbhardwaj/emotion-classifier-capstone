# MLP Fusion Config - Fair Comparison (50 Epochs, ~8M Params)
# ============================================================

# CRITICAL: Architecture selection
architecture_name: mlp_fusion

# Experiment settings
experiment_name: colab_run_mlp_fusion_fair_comparison
random_seed: 42

# Data paths
data_root: /content/drive/MyDrive/dlfa_capstone/meld_data

# Model architecture settings
output_dim: 7
mlp_hidden_size: 4096           # INCREASED: 2048â†’4096 for ~8M comparable params
mlp_dropout_rate: 0.3

# Encoder settings - FAIR COMPARISON
text_encoder_model_name: roberta-base
audio_encoder_model_name: facebook/wav2vec2-base-960h
audio_input_type: raw_wav
freeze_text_encoder: false     # UNFROZEN: for fair comparison
freeze_audio_encoder: false    # UNFROZEN: for fair comparison

# Fine-tuning settings - STANDARDIZED for all models
fine_tune:
  audio_encoder:
    unfreeze_top_n_layers: 12
    lr_mul: 1.5
  text_encoder:
    unfreeze_top_n_layers: 12
    lr_mul: 1.5

# Training settings - STANDARDIZED
batch_size: 8                   # Consistent across models
num_epochs: 50                  
learning_rate: 2e-5             # Conservative for unfrozen encoders
weight_decay: 1e-4
optimizer_name: AdamW
gradient_clip_val: 0.5          # Conservative for stability

# Loss settings
focal_gamma: 2.0

# Class weights for MELD emotion imbalance
class_weights: [1.62, 8.22, 11.19, 1.40, 0.45, 2.69, 2.00]

# Training options
use_mixed_precision: true
early_stopping_patience: 10    
grad_accumulation_steps: 1

# DataLoader settings
dataloader_num_workers: 2

# Text processing
text_max_length: 128

# Scheduler settings
eta_min: 1e-7 