{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU and auto-reload\n",
    "# (In the Colab UI: Runtime ▷ Change runtime type ▷ GPU)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')              # → enter OAuth code\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/dlfa_capstone'\n",
    "DATA_ROOT  = f'{DRIVE_ROOT}/meld_data'\n",
    "CKPT_ROOT  = f'{DRIVE_ROOT}/checkpoints'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone or Pull Repo\n",
    "import pathlib\n",
    "\n",
    "REPO_URL = \"https://github.com/mehulbhardwaj/emotion-classifier-capstone.git\"\n",
    "REPO_DIR = \"/content/emotion-classifier-capstone\"\n",
    "\n",
    "if pathlib.Path(REPO_DIR).exists():\n",
    "    %cd $REPO_DIR\n",
    "    !git pull --quiet\n",
    "else:\n",
    "    !git clone $REPO_URL $REPO_DIR --quiet\n",
    "    %cd $REPO_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL TO DOWNLOAD MELD RAW DATA (CSVs and MP4s)\n",
    "# This cell uses scripts/download_meld_dataset.py to fetch the complete\n",
    "\n",
    "\n",
    "print(\"Starting MELD Raw Dataset Download Process...\")\n",
    "\n",
    "DRIVE_RAW_MELD_PATH = f\"{DRIVE_ROOT}/meld_data/raw/\" \n",
    "\n",
    "print(f\"Target directory for raw MELD data (CSVs, MP4s): {DRIVE_RAW_MELD_PATH}\")\n",
    "\n",
    "# Create the target directory if it doesn't exist to avoid issues with the script\n",
    "import os\n",
    "os.makedirs(DRIVE_RAW_MELD_PATH, exist_ok=True)\n",
    "\n",
    "# Run the download script.\n",
    "# This script handles downloading the main MELD.Raw.tar.gz, extracting its contents (CSVs, video tarballs),\n",
    "# and then extracting the MP4 videos from those inner tarballs.\n",
    "# You can add --force_download_main or --force_extract_videos if you need to re-run parts of the process.\n",
    "%cd /content/emotion-classifier-capstone\n",
    "!python scripts/download_meld_dataset.py --data_dir \"{DRIVE_RAW_MELD_PATH}\"\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------\")\n",
    "print(\"MELD Raw Data Download and Initial Extraction Process Finished.\")\n",
    "print(f\"All downloaded and extracted raw data should be available in: {DRIVE_RAW_MELD_PATH}\")\n",
    "print(\"1. Convert these MP4 videos to WAV audio files (e.g., using a script like preprocess_meld.py).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3 – Install dependencies via pip\n",
    "!pip install -U pip wheel\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Cache HF models & datasets on Drive\n",
    "import os\n",
    "os.environ[\"HF_HOME\"]        = f\"{DRIVE_ROOT}/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = f\"{DRIVE_ROOT}/hf_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 Revised\n",
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "# These are from your Cell 2\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/dlfa_capstone'\n",
    "DATA_ROOT  = f'{DRIVE_ROOT}/datasets/meld_data'\n",
    "CKPT_ROOT  = f'{DRIVE_ROOT}/checkpoints'\n",
    "\n",
    "colab_mlp_fusion_yaml_content = f\"\"\"\n",
    "architecture_name: \"mlp_fusion\"\n",
    "experiment_name: \"mlp_fusion_colab_run\" # Specific experiment name for this run\n",
    "\n",
    "paths:\n",
    "  data_root: {DATA_ROOT} # Points to your Drive path\n",
    "  output_dir_root: {CKPT_ROOT} # Points to your Drive path\n",
    "\n",
    "# --- General Project Settings ---\n",
    "dataset_name: \"meld\"\n",
    "input_mode: \"audio_text\" # Assuming this is what mlp_fusion uses\n",
    "random_seed: 42\n",
    "device_name: \"cuda\" # Colab provides GPU\n",
    "# num_dataloader_workers: 2 # Set a reasonable default for Colab\n",
    "\n",
    "# --- Data Preparation (Ensure these are false if using existing dataset) ---\n",
    "run_mp4_to_wav_conversion: false\n",
    "run_hf_dataset_creation: false\n",
    "\n",
    "# --- Training Parameters (copy from your local mlp_fusion_default.yaml or adjust) ---\n",
    "num_epochs: 1 # As per your Cell 5\n",
    "batch_size: 16 # As per your Cell 5\n",
    "learning_rate: 0.00003 # Adjusted from your 3e-4, check if this is intended\n",
    "optimizer_name: \"AdamW\"\n",
    "lr_scheduler_name: \"linear\" # Or null if not used\n",
    "# ... other training params from your local mlp_fusion_default.yaml ...\n",
    "\n",
    "# --- Dataset Specific Limits ---\n",
    "limit_dialogues_train: 50 # As per your Cell 5\n",
    "limit_dialogues_dev: 10   # As per your Cell 5\n",
    "limit_dialogues_test: 10  # As per your Cell 5\n",
    "\n",
    "# --- MLP Fusion Specific Parameters (Copy from your local mlp_fusion_default.yaml) ---\n",
    "text_encoder_model_name: \"distilroberta-base\"\n",
    "audio_encoder_model_name: \"facebook/wav2vec2-base-960h\"\n",
    "mlp_hidden_size: 768 # Or whatever your model expects\n",
    "mlp_dropout_rate: 0.1 # Or whatever your model expects\n",
    "\n",
    "# Add any other necessary parameters from your local mlp_fusion_default.yaml\n",
    "\"\"\"\n",
    "\n",
    "colab_config_path = pathlib.Path(\"configs/mlp_fusion_colab.yaml\")\n",
    "colab_config_path.write_text(colab_mlp_fusion_yaml_content)\n",
    "print(f\"Generated Colab config at: {colab_config_path}\")\n",
    "print(\"--- Config Content ---\")\n",
    "print(colab_mlp_fusion_yaml_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5 – Run (or resume) training\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# This picks up `last.ckpt` (if present), writes new ckpts every 500 steps,\n",
    "# and now cfg.experiment_name is guaranteed to exist.\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "!python main.py \\\n",
    "   --config_file configs/mlp_fusion_colab.yaml \\\n",
    "   --architecture mlp_fusion \\\n",
    "   --train_model \\\n",
    "   --num_epochs 1 \\\n",
    "   --limit_dialogues_train 50 \\\n",
    "   --limit_dialogues_dev 10 \\\n",
    "   --limit_dialogues_test 10 \\\n",
    "   --batch_size 16 \\\n",
    "   --learning_rate 3e-4 \\\n",
    "   --experiment_name mlp_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6 – Validation / Inference\n",
    "!python main.py \\\n",
    "   --config_file configs/path_colab.yaml \\\n",
    "   --evaluate_model {CKPT_ROOT}/mlp_fusion/last.ckpt\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
