# -*- coding: utf-8 -*-
"""Emotion Classification DLFA Capstone _ dialog-rnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d0M0nasfn7f4UpO3sJbVlilGvvfLq4qF
"""

# ⬇️ BLOCK-0 – dependencies
!apt-get -qq update
!apt-get -qq install -y ffmpeg libsndfile1   # audio

# Commented out IPython magic to ensure Python compatibility.
import pathlib

REPO_URL = "https://github.com/mehulbhardwaj/emotion-classifier-capstone.git"
REPO_DIR = "/content/emotion-classifier-capstone"

if pathlib.Path(REPO_DIR).exists():
#     %cd $REPO_DIR
    !git pull --quiet
else:
    !git clone $REPO_URL $REPO_DIR --quiet
#     %cd $REPO_DIR

!pip install -q -r requirements.txt

from google.colab import drive; drive.mount('/content/drive')
DRIVE_ROOT = "/content/drive/MyDrive/dlfa_capstone"



# (OPTIONAL) extract features abd build hf datasets if needed
"""
!python scripts/build_hf_dataset.py \
        --csv_dir   "$RAW_CSV_DIR" \
        --wav_dir   "$WAV_DIR" \
        --out_dir   "$FEATURE_DIR" \
        --sample_rate 16000 \
        --num_proc 4

# train
!python train.py --config configs/colab_config_dialog_rnn.yaml

# update config_path to match model in the sanity_check script      # or _todkat_lite.yaml, _fusion.yaml
!PYTHONPATH=/content/emotion-classifier-capstone:$PYTHONPATH \
  python scripts/sanity_check.py

!python scripts/extract_embeddings.py \
    --ckpt /content/drive/MyDrive/dlfa_capstone/checkpoints/dialog_rnn/last.ckpt \
    --config configs/colab_config_dialog_rnn.yaml \
    --splits train dev test \
    --out_dir /content/drive/MyDrive/dlfa_capstone/cached_emb/

# 1️⃣  Standard imports -------------------------------------------------
from pathlib import Path
import yaml
from omegaconf import OmegaConf   # already installed by lightning
from utils.data_processor import MELDDataModule

# 2️⃣  Load the same YAML you pass on the CLI --------------------------
cfg_path = Path("configs/colab_config_dialog_rnn.yaml")
config   = OmegaConf.load(cfg_path)          # simple dict-like object

defaults = {
    "text_encoder_model_name":  "roberta-base",
    "audio_encoder_model_name": "facebook/wav2vec2-base-960h",
    "audio_input_type":         "raw_wav",      # or "hf_features"
    "output_dim":               7,
}
for k, v in defaults.items():
    if k not in config:
        config[k] = v

# 3️⃣  Build the DataModule exactly as train.py does -------------------
dm = MELDDataModule(config)
dm.setup("fit")                  # builds train + dev splits

print("train :", len(dm.datasets["train"]))
print("dev   :", len(dm.datasets["dev"]))
print("test  :", len(dm.datasets["test"]))

import torch

# if you built `config` (Option A) do this ⬇︎
output_dim = getattr(config, "output_dim", 7)                # fallback = 7 classes
batch      = next(iter(dm.val_dataloader()))
print("Label counts in first batch:", torch.bincount(batch[-1], minlength=output_dim))

!python evaluate.py \
  --checkpoint /content/drive/MyDrive/dlfa_capstone/checkpoints/dialog_rnn/last.ckpt \
  --config    configs/colab_config_dialog_rnn.yaml

# Commented out IPython magic to ensure Python compatibility.
#Visualise training logs

# %load_ext tensorboard
# %tensorboard --logdir /content/drive/MyDrive/dlfa_capstone/tb_logs

