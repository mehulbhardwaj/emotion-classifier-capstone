#!/usr/bin/env python3
"""
Test script for the corrected TOD-KAT sequence-to-sequence implementation.
Verifies that the model now predicts emotions for all utterances in a dialogue
rather than just the last one.
"""

import sys
import torch
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from configs.base_config import BaseConfig
from models.todkat_lite import TodkatLiteMLP


def create_test_batch(batch_size=2, seq_len=5, audio_len=1000, text_len=20):
    """Create a test batch with known valid/invalid patterns."""
    
    # Create a batch where:
    # - Batch 0: 3 valid utterances (positions 0,1,2), 2 padding (positions 3,4)
    # - Batch 1: 4 valid utterances (positions 0,1,2,3), 1 padding (position 4)
    
    batch = {
        "wav": torch.randn(batch_size, seq_len, audio_len),
        "wav_mask": torch.ones(batch_size, seq_len, audio_len, dtype=torch.bool),
        "txt": torch.randint(1, 1000, (batch_size, seq_len, text_len)),  # Avoid 0 (pad)
        "txt_mask": torch.ones(batch_size, seq_len, text_len, dtype=torch.bool),
        "topic_id": torch.randint(0, 50, (batch_size, seq_len)),
        "kn_vec": torch.randn(batch_size, seq_len, 50),  # Optional knowledge vectors
    }
    
    # Dialog mask: True for valid utterances, False for padding
    dialog_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)
    dialog_mask[0, :3] = True  # First dialogue has 3 valid utterances
    dialog_mask[1, :4] = True  # Second dialogue has 4 valid utterances
    batch["dialog_mask"] = dialog_mask
    
    # Labels: valid emotions (0-6) for valid utterances, -1 for padding
    labels = torch.full((batch_size, seq_len), -1, dtype=torch.long)
    labels[0, :3] = torch.randint(0, 7, (3,))  # Valid labels for first dialogue
    labels[1, :4] = torch.randint(0, 7, (4,))  # Valid labels for second dialogue
    batch["labels"] = labels
    
    return batch


def test_todkat_shapes():
    """Test that TOD-KAT produces correct output shapes."""
    print("üîç Testing TOD-KAT output shapes...")
    
    # Create config
    config = BaseConfig()
    config.architecture_name = "todkat_lite"
    config.output_dim = 7
    config.topic_embedding_dim = 100
    config.n_topics = 50
    config.rel_transformer_layers = 2
    config.rel_heads = 4
    config.use_knowledge = True
    
    # Create model
    model = TodkatLiteMLP(config)
    model.eval()
    
    # Create test batch
    batch = create_test_batch(batch_size=2, seq_len=5)
    
    print(f"Input shapes:")
    for key, value in batch.items():
        print(f"  {key}: {value.shape}")
    
    # Forward pass
    with torch.no_grad():
        logits = model(batch)
    
    print(f"\nOutput logits shape: {logits.shape}")
    print(f"Expected shape: (batch_size=2, seq_len=5, num_classes=7)")
    
    assert logits.shape == (2, 5, 7), f"Expected shape (2, 5, 7), got {logits.shape}"
    print("‚úÖ Shape test passed!")
    
    return batch, model


def test_causal_masking():
    """Test that causal masking works correctly."""
    print("\nüîç Testing causal masking...")
    
    # Create a simple test where we can verify causality
    config = BaseConfig()
    config.architecture_name = "todkat_lite"
    config.output_dim = 7
    config.topic_embedding_dim = 10  # Smaller for testing
    config.n_topics = 5
    config.rel_transformer_layers = 1  # Single layer for easier testing
    config.rel_heads = 1
    config.use_knowledge = False  # Disable for simpler testing
    
    model = TodkatLiteMLP(config)
    model.eval()
    
    # Create a batch with specific pattern
    batch_size, seq_len = 1, 3
    batch = create_test_batch(batch_size=batch_size, seq_len=seq_len)
    
    # Make all utterances valid
    batch["dialog_mask"][:] = True
    batch["labels"][:] = torch.randint(0, 7, (batch_size, seq_len))
    
    # Test forward pass
    with torch.no_grad():
        logits = model(batch)
    
    print(f"Causal output shape: {logits.shape}")
    print("‚úÖ Causal masking test completed!")
    
    return logits


def test_training_step():
    """Test that training step works with the new implementation."""
    print("\nüîç Testing training step...")
    
    config = BaseConfig()
    config.architecture_name = "todkat_lite"
    config.output_dim = 7
    config.topic_embedding_dim = 100
    config.n_topics = 50
    config.rel_transformer_layers = 2
    config.rel_heads = 4
    config.use_knowledge = False  # Simpler for testing
    config.focal_gamma = 2.0
    config.class_weights = [1.0] * 7
    
    model = TodkatLiteMLP(config)
    model.train()
    
    # Create test batch
    batch = create_test_batch(batch_size=2, seq_len=5)
    
    print(f"Dialog mask sum: {batch['dialog_mask'].sum()} valid utterances")
    print(f"Valid labels: {batch['labels'][batch['dialog_mask']]}")
    
    # Test training step
    try:
        loss = model._shared_step(batch, "train")
        print(f"Training loss: {loss.item():.4f}")
        print("‚úÖ Training step test passed!")
        
        # Test backward pass
        loss.backward()
        print("‚úÖ Backward pass test passed!")
        
    except Exception as e:
        print(f"‚ùå Training step failed: {e}")
        raise
    
    return loss


def test_data_efficiency():
    """Test that we're using more data now (all valid utterances vs just last)."""
    print("\nüîç Testing data efficiency...")
    
    config = BaseConfig()
    config.architecture_name = "todkat_lite"
    config.output_dim = 7
    config.topic_embedding_dim = 50
    config.n_topics = 50
    config.rel_transformer_layers = 1
    config.rel_heads = 2
    config.use_knowledge = False
    config.focal_gamma = 2.0
    config.class_weights = [1.0] * 7
    
    model = TodkatLiteMLP(config)
    model.eval()
    
    # Create batch with many valid utterances
    batch_size, seq_len = 4, 8
    batch = create_test_batch(batch_size=batch_size, seq_len=seq_len)
    
    # Make varying dialogue lengths
    batch["dialog_mask"][0, :6] = True  # 6 valid utterances
    batch["dialog_mask"][1, :4] = True  # 4 valid utterances  
    batch["dialog_mask"][2, :8] = True  # 8 valid utterances
    batch["dialog_mask"][3, :2] = True  # 2 valid utterances
    
    # Set valid labels
    for i in range(batch_size):
        valid_len = batch["dialog_mask"][i].sum().item()
        batch["labels"][i, :valid_len] = torch.randint(0, 7, (valid_len,))
    
    total_valid_utterances = batch["dialog_mask"].sum().item()
    print(f"Total valid utterances in batch: {total_valid_utterances}")
    print(f"Old approach would use only: {batch_size} (last utterances)")
    print(f"New approach uses: {total_valid_utterances} utterances")
    print(f"Data efficiency improvement: {total_valid_utterances / batch_size:.1f}x")
    
    # Test that loss computation works
    with torch.no_grad():
        try:
            loss = model._shared_step(batch, "test")
            print(f"Loss computed successfully: {loss.item():.4f}")
            print("‚úÖ Data efficiency test passed!")
        except Exception as e:
            print(f"‚ùå Data efficiency test failed: {e}")
            raise


def main():
    """Run all tests."""
    print("üöÄ Testing corrected TOD-KAT implementation")
    print("=" * 60)
    
    # Test 1: Shape verification
    batch, model = test_todkat_shapes()
    
    # Test 2: Causal masking
    test_causal_masking()
    
    # Test 3: Training step
    test_training_step()
    
    # Test 4: Data efficiency
    test_data_efficiency()
    
    print("\n" + "=" * 60)
    print("üéâ All tests passed! TOD-KAT implementation is working correctly.")
    print("\nKey improvements:")
    print("‚úÖ Sequence-to-sequence prediction for all utterances")
    print("‚úÖ Causal masking for autoregressive prediction")
    print("‚úÖ Proper handling of dialogue-level batching")
    print("‚úÖ Significantly improved data efficiency")
    print("‚úÖ Follows original TOD-KAT paper methodology")


if __name__ == "__main__":
    main() 